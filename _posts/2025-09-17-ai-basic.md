---
layout: single
title: "深度学习总结-大框架"
date: 2025-09-17 15:00:00 +0800
categories: [ai]
tags: [deep_learning, summary]
toc: true
---

## 个人总结
有时候会忘记了整个机器学习总体在做什么，the big picture，所以本篇2025.09.17读依然觉得很有意思很有收获。

对于接受次优解这一特点，现在有很深刻的体会，因为我总是会问为什么有效/为什么选择这个参数，你怎么知道它是最优的，比如layer数，比如learning rate，比如论文中的超参数，审稿我都会问how choose。实际上我们的优化算法不需要找到全局最优，只需要局部最优；我们的lr=0.1可能最终效果比lr=0.08差了1%，但是lr=0.1也足够好了，不需要穷举出最优的参数；我们发论文提出一个方法，不需要找到最优的超参数来达到最优的性能，只需要找出一组超参数，使我们的方法比现有方法效果更好，并且能够对领域产生一个新的思路/新的理解，这就可以了。可能说的不好，总之还需要再继续理解。


## 机器学习概念介绍
机器学习之前计算机程序：核心在于业务逻辑，详细说明应用程序在各种情况、各种边界条件下的操作和处理。通过测试一次次更新边界条件，最终完成自动化程序的过程。即从输入到输出的所有过程都精准编写。

机器学习：从经验中学习的技术。通过数据学习经验，完成自动化任务的过程。输入输出不变，但是中间过程由模型（确定的参数parameter）确定，不再由人工编写的一个个业务逻辑决定。

深度学习：机器学习中的一个分支。利用神经网络，完成特征自动提取和分类的过程，代替了传统机器学习中“特征提取”与“分类任务”分开的过程。

## 机器学习的关键组件

### 数据 data：用来学习
- 一般遵循独立同分布
- 每个数据点由(feature, label)组成
- 数据的正确性很重要：偏误，分布，都会对模型最终的效果有影响。

### 模型 model：转换数据
- 深度学习：由神经网络组成，包括层层数据转换。

### 目标函数 obj func：量化模型的有效性
- 度量模型的优越程度，也就是对目标的描述。
- 最常见的目标函数：平方误差。

### 学习算法 learning algo：调整模型参数以优化目标函数
- 有了数据、模型和目标函数，需要有一套规则来优化模型，提高目标函数的效果。
- 所有优化算法都基于梯度下降 gradient descent。

## 机器学习问题分类

### 监督学习 supervised learning
数据由（特征，标签）样本对组成。模型的目标是将特征映射到标签。
类别：
- 回归 regression：标签取任意数值的监督学习问题。模型输出就是标签值。
- 分类 classification：标签为离散的数值。但是我们的输出并不是标签值，而是(标签类,1)的向量，每个元素为[0,1]的概率。
- 多标签分类 multi-label classification：一个物体归属于多个标签类别。
- 搜索
- 推荐系统
- 序列学习：输入是连续的，模型需要拥有记忆功能，记得过去的数据，以成功预测未来。
    - 自动语音识别；文本到语音；机器翻译；

### 无监督学习 unsupervised learning
没有对目标有要求，也没有标签。
- 聚类 clustering：对数据进行分类
- 主成分分析 PCA：用少维特征线性表示特征。
- 因果关系/概率图模型：探索相关关系？
- 生成对抗性网络 GAN：为啥是无监督学习？

### 与环境互动/强化学习
监督/无监督都是离线学习，即获取完数据以后，模型和环境没有交互了。

与真实环境互动，会影响未来的观察结果，让问题更难。

智能体：在一系列时间步骤上与环境交互的机器学习模型。每个时刻点，智能体从环境中得到观察observation，根据观察做出动作action，传输给环境（有时候通过执行器），最后智能体获得奖励reward。

![强化学习基础框架](/assets/img/ai/rein_learning.png)

强化学习的目标是产生一个好的策略policy，即从环境观察映射到行为的功能。

强化学习框架具备通用性，比如监督学习的目标函数/损失函数就可以认为是奖励。

## 机器学习发展
早期方法：
- 多层感知机MLP
- 卷积神经网络CNN
- 长短期记忆网络LSTM
- Q学习

近十年（具体贡献还需要体会）：
- 容量控制方法，如dropout，减轻过拟合风险
- 注意力机制：不增加可学习参数的情况下增加系统的记忆和复杂性（没懂啥意思，回顾Transformer看看和RNN对比，似乎是参数增多了呀）
- 多阶段设计，如encoder-decoder。允许重复修改深度神经网络的内容状态，执行后续步骤（encoder得到向量后，可以不断接收输入给decoder，不断输出）
- GAN：打开密度估计的大门。
- 多GPU并行计算
- 深度学习框架

## 深度学习的特点
- 表示学习 representation learning:深度学习通过层层堆叠，每层都是一种表示，比如靠近输入是低级细节，靠近分类则是抽象概念。深度学习就是不断对输入进行自动的特征抽取，可以认为是多级表示学习。
- 端到端训练：不需要对特征进行人工抽取，直接以整个系统的角度进行性能优化。代替了特征工程的过程。
- 从参数统计描述到完全非参数模型的转变（没懂）
- **接受次优解**：愿意处理非凸非线性优化问题，并且愿意在证明之前尝试。以前都是追求理论的优美，最优解的证明，但是现在的思路是经验主义，以实用为目标，不追求理论证明完备/最优解。
    - 比如epoch收敛数目，只要（接近）平滑，不需要确定它是最优解。
    - 比如ResNet，提出了residual connection，empirical evidence很多说明方法有效，但是为什么有效/证明why并不必须（优秀的论文只需要给出一个可能的合理解释即可），即大量实验证明方法确实有效，再加上自圆其说的解释。
- 开源





## 参考
- [d2l-引言](https://zh.d2l.ai/chapter_introduction/index.html)